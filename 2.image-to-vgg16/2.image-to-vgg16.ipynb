{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VGG16 Image Embeddings\n",
    "\n",
    "_created by Austin Poor_\n",
    "\n",
    "In this notebook, I use a pretrained VGG-16 model to create image embeddings for each of the film stills.\n",
    "\n",
    "The notebook [1.format-images.ipynb](./1.format-images.ipynb), has placed uniform images in an S3 bucket for this notebook to pull down, process, and then upload the results (as individual parquet files) to another S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dir = Path(\"./tmp\")\n",
    "tmp_dir.mkdir(exist_ok=True)\n",
    "[f.unlink() for f in tmp_dir.glob(\"*\") if f.is_file()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_BUCKET = \"apoor-clean-movie-stills\"\n",
    "DEST_BUCKET = \"apoor-vgg-movie-vecs\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1_000 # Max of 1,000 per S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 300, 300, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 300, 300, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 150, 150, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (500, 500, 3)\n",
    "\n",
    "vgg16 = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=input_shape\n",
    ")\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_keys(bucket: str, batch_size: int = 1_000) -> [str]:\n",
    "    \"\"\"Iterates through keys in an S3 `bucket`\n",
    "    in groups of `batch_size`\n",
    "\n",
    "    :param bucket: Name of the S3 bucket to search\n",
    "    :param batch_size: The max number of keys to return at a time.\n",
    "        Note: S3 will return a maximum of 1,000 keys at a time.\n",
    "    :yields: A list of keys to files in the S3 bucket, `bucket`\n",
    "    \"\"\"\n",
    "    last_key = \"\"\n",
    "    while True:\n",
    "        resp = s3.list_objects_v2(\n",
    "            Bucket=SOURCE_BUCKET,\n",
    "            MaxKeys=batch_size,\n",
    "            StartAfter=last_key\n",
    "        )\n",
    "        keys = [c[\"Key\"] for c in resp[\"Contents\"]]\n",
    "        yield keys\n",
    "        if not resp[\"IsTruncated\"]: break\n",
    "        else: last_key = keys[-1]\n",
    "            \n",
    "            \n",
    "def download_object(bucket: str, key: str, tmp_dir: Path) -> Path:\n",
    "    \"\"\"Downloads a single file from an S3 bucket\n",
    "    and stores it in a temporary directory\n",
    "\n",
    "    :param bucket: The bucket to search in S3\n",
    "    :param key: The object's key in `bucket`\n",
    "    :param tmp_dir: The temporary directory to save the\n",
    "        downloaded file.\n",
    "    :returns: A path to the downloaded file in `tmp_dir`\n",
    "    \"\"\"\n",
    "    res = s3.get_object(Bucket=bucket, Key=key)\n",
    "    filename = tmp_dir / key\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(res[\"Body\"].read())\n",
    "    return filename\n",
    "    \n",
    "    \n",
    "def batch_download(bucket: str, keys: [str], tmp_dir: Path) -> [Path]:\n",
    "    \"\"\"Downloads a batch of objects from an S3 bucket.\n",
    "\n",
    "    Uses the function `download_image` in a multi-threaded\n",
    "    map -- using `concurrent.futures.ThreadPoolExecutor`.\n",
    "\n",
    "    :param bucket: S3 bucket where objects are stored\n",
    "    :param keys: List of object keys stored in `bucket`\n",
    "    :param tmp_dir: Local directory to save downloaded images\n",
    "    :returns: List of paths for locally stored objects\n",
    "        downloaded from S3.\n",
    "    \"\"\"\n",
    "    def curried_download(key: str): \n",
    "        return download_object(bucket,key,tmp_dir)\n",
    "    with ThreadPoolExecutor() as P:\n",
    "        return list(P.map(curried_download,keys))\n",
    "\n",
    "    \n",
    "def clean_tmp_files(paths: [Path]):\n",
    "    \"\"\"Unlinks a list of files using `pathlib.Path.unlink`.\n",
    "\n",
    "    :param paths: List of paths to files that should be deleted.\n",
    "    \"\"\"\n",
    "    [Path(p).unlink() for p in paths]\n",
    "    \n",
    "    \n",
    "def load_image(path: Path) -> np.ndarray:\n",
    "    \"\"\"Loads a JPEG image at `path` as an ndarray.\n",
    "\n",
    "    Uses `tf.io.read_file` and `tf.image.decode_jpeg`\n",
    "\n",
    "    :param path: Path to an image file\n",
    "    :returns: ndarray representation of the image\n",
    "        with dimensions (img_witdh,img_height,img_color_channels)\n",
    "    \"\"\"\n",
    "    return tf.image.decode_jpeg(tf.io.read_file(str(path)))\n",
    "\n",
    "    \n",
    "def load_images(paths: [Path]) -> np.ndarray:\n",
    "    \"\"\"Loads a group of images as numpy arrays,\n",
    "    and concatenates them together.\n",
    "\n",
    "    :param paths: List of paths to image files being loaded\n",
    "    :returns: A single numpy ndarray with dimensions\n",
    "        (n_image,img_witdh,img_height,img_color_channels)\n",
    "    \"\"\"\n",
    "    return np.concatenate([\n",
    "        np.expand_dims(load_image(p),0)\n",
    "        for p in paths\n",
    "    ],0)\n",
    "\n",
    "\n",
    "def format_input(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Formats image for processing using `tf.keras`'s \n",
    "    supplied function for preprocessing VGG16 inputs.\n",
    "    \n",
    "    :param data: ndarray of images with shape: (n,w,h,c)\n",
    "    :returns: ndarray with images rescaled / typed to\n",
    "        match what VGG16 expects.\n",
    "    \"\"\"\n",
    "    return preprocess_input(data)\n",
    "\n",
    "\n",
    "def vgg_process(data: np.ndarray):\n",
    "    \"\"\"Creates VGG-16 embeddings from image data.\n",
    "\n",
    "    :param data: ndarray of image data\n",
    "    :returns: ndarray of image embeddings\n",
    "    \"\"\"\n",
    "    return vgg16.predict(data)\n",
    "\n",
    "\n",
    "def format_output(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Reformats image embeddings for parquet\n",
    "    storrage.\n",
    "\n",
    "    :param data: ndarray of image embeddings (4-dimensional)\n",
    "    :returns: (Mostly-)flattened ndarray with dimensions:\n",
    "        (n_images, flattened_vgg_output)\n",
    "    \"\"\"\n",
    "    batch_size, *_ = data.shape\n",
    "    return data.reshape((batch_size, -1))\n",
    "\n",
    "\n",
    "def make_arrow_table(row: np.ndarray, filename: Path) -> pa.Table:\n",
    "    \"\"\"Converts a flat numpy array into an arrow Table,\n",
    "    where the key is `filename`'s stem and the value is\n",
    "    the row of data.\n",
    "\n",
    "    :param row: A flat numpy array\n",
    "    :param filename: A path where the filename's stem will become\n",
    "        the key in the arrow table\n",
    "    :returns: An arrow table with `filename`'s stem as a key\n",
    "        and `row` as the data\n",
    "    \"\"\"\n",
    "    return pa.table({Path(filename).stem: row})\n",
    "\n",
    "\n",
    "def write_parquet(row: np.ndarray, filename: Path) -> Path:\n",
    "    \"\"\"Writes the data in `row` as an arrow table,\n",
    "    to a parquet file.\n",
    "\n",
    "    Saves the file to `filename` where the extension\n",
    "    is changed to `.parquet`.\n",
    "\n",
    "    In the arrow table, `row`'s key is the stem of\n",
    "    `filename`.\n",
    "\n",
    "    For example, if `filename = \"dir/test.jpg\"` then\n",
    "    the result will be a parquet file: `dir/test.parquet`\n",
    "    which stores an arrow table with the key `test`.\n",
    "\n",
    "    :param row: Data to be stored in a parquet file\n",
    "    :param filename: Source data's filename\n",
    "    :returns: Path to the newly created parquet file\n",
    "    \"\"\"\n",
    "    table = make_arrow_table(row, filename)\n",
    "    new_filename = filename.with_suffix(\".parquet\")\n",
    "    pq.write_table(table, new_filename)\n",
    "    return new_filename\n",
    "\n",
    "\n",
    "def write_parquets(data: np.ndarray, filenames: [Path]) -> [Path]:\n",
    "    \"\"\"Writes the rows in `data` to parquet files\n",
    "    based of the paths in the list, `filenames`.\n",
    "\n",
    "    The rows in `data` should correspond to the paths\n",
    "    in `filenames`.\n",
    "\n",
    "    See `write_parquet` for more details.\n",
    "\n",
    "    :param data: a 2-D ndarray with data to be stored\n",
    "        as parquet files.\n",
    "    :param filenames: List of source `Paths` corresponding\n",
    "        to the rows in `data`.\n",
    "    :returns: A list of `Paths` to the newly created parquet files\n",
    "    \"\"\"\n",
    "    return [write_parquet(r, f) for r, f in zip(data, filenames)]\n",
    "\n",
    "def upload_parquet_file(bucket: str, filename: Path):\n",
    "    \"\"\"Upload a parquet file to S3.\n",
    "\n",
    "    The key used in S3 will be the name and extension\n",
    "    from filename (aka no directory names included).\n",
    "\n",
    "    For example, if `filename = \"path/to/file.ext\"`, then\n",
    "    the object's key in S3 will be `file.ext`.\n",
    "\n",
    "    :param bucket: S3 bucket to store the file\n",
    "    :param filename: File to be stored in S3\n",
    "    \"\"\"\n",
    "    key = filename.name\n",
    "    s3.upload_file(str(filename), bucket, key)\n",
    "\n",
    "\n",
    "def upload_parquet_files(bucket: str, filenames: [Path]):\n",
    "    \"\"\"Upload multiple parquet files to S3.\n",
    "\n",
    "    Uses the function `upload_parquet_file` in a multi-threaded \n",
    "    map with `concurrent.futures.ThreadPoolExecutor`.    \n",
    "\n",
    "    :param bucket: S3 bucket to upload files\n",
    "    :param filenames: List of `Path`s to parquets being uploaded\n",
    "    \"\"\"\n",
    "    def curried_upload(filename):\n",
    "        upload_parquet_file(bucket, filename)\n",
    "\n",
    "    with ThreadPoolExecutor() as P:\n",
    "        list(P.map(curried_upload, filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = dt.datetime.now()\n",
    "print(f\"START TIME: {start_time}\")\n",
    "print(f\"Loading batches of {batch_size:,d} images.\\n\")\n",
    "\n",
    "bar = tqdm(\n",
    "    enumerate(iter_keys(SOURCE_BUCKET, batch_size)),\n",
    "    desc=\"Starting...\",\n",
    "    unit=\"batches\",\n",
    "    ncols=80\n",
    ")\n",
    "\n",
    "for i, image_keys in bar:\n",
    "    loop_start = dt.datetime.now()\n",
    "    bar.set_description(\"Downloading images\")\n",
    "    image_paths = batch_download(SOURCE_BUCKET, image_keys, tmp_dir)\n",
    "\n",
    "    bar.set_description(\"Loading images\")\n",
    "    input_data = load_images(image_paths)\n",
    "    input_data = format_input(input_data)\n",
    "\n",
    "    bar.set_description(\"Removing tmp files\")\n",
    "    clean_tmp_files(image_paths)\n",
    "\n",
    "    bar.set_description(\"Embedding with VGG16\")\n",
    "    encoding = vgg_process(input_data)\n",
    "    output_data = format_output(encoding)\n",
    "    \n",
    "    bar.set_description(\"Saving to parquet\")\n",
    "    parquet_paths = write_parquets(output_data, image_paths)\n",
    "\n",
    "    print(\"> Uploading encodings...\")\n",
    "    bar.set_description(\"Uploading embeddings\")\n",
    "    upload_parquet_files(DEST_BUCKET, parquet_paths)\n",
    "\n",
    "    bar.set_description(\"Removing tmp files\")\n",
    "    clean_tmp_files(parquet_paths)\n",
    "    \n",
    "    bar.write(f\"[{i:4,d}] COMPLETED IN {dt.datetime.now() - loop_start}\")\n",
    "\n",
    "print(f\"\\nFULL TIME TO COMPLETE: {dt.datetime.now() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
