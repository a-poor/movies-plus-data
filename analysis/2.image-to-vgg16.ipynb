{
 "cells": [
  {
   "source": [
    "# 2. VGG16 Image Embeddings\n",
    "\n",
    "_created by Austin Poor_\n",
    "\n",
    "In this notebook, I use a pretrained VGG-16 model to create image embeddings for each of the film stills.\n",
    "\n",
    "The notebook [1.format-images.ipynb](./1.format-images.ipynb), has placed uniform images in an S3 bucket for this notebook to pull down, process, and then upload the results (as individual parquet files) to another S3 bucket."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tmp_dir = Path(\"./tmp\")\n",
    "tmp_dir.mkdir(exist_ok=True)\n",
    "[f.unlink() for f in tmp_dir.glob(\"*\") if f.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_BUCKET = \"apoor-clean-movie-stills\"\n",
    "DEST_BUCKET = \"apoor-vgg-movie-vecs\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 # Max of 1,000 per S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 300, 300, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 300, 300, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 300, 300, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 150, 150, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 150, 150, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 150, 150, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 75, 75, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 75, 75, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 75, 75, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 75, 75, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (300, 300, 3)\n",
    "\n",
    "vgg16 = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=input_shape\n",
    ")\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_keys(bucket: str, batch_size: int = 1_000):\n",
    "    last_key = \"\"\n",
    "    while True:\n",
    "        resp = s3.list_objects_v2(\n",
    "            Bucket=SOURCE_BUCKET,\n",
    "            MaxKeys=batch_size,\n",
    "            StartAfter=last_key\n",
    "        )\n",
    "        keys = [c[\"Key\"] for c in resp[\"Contents\"]]\n",
    "        yield keys\n",
    "        if not resp[\"IsTruncated\"]: break\n",
    "        else: last_key = keys[-1]\n",
    "            \n",
    "            \n",
    "def download_object(bucket: str, key: str, tmp_dir: Path) -> Path:\n",
    "    res = s3.get_object(Bucket=bucket, Key=key)\n",
    "    filename = tmp_dir / key\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(res[\"Body\"].read())\n",
    "    return filename\n",
    "    \n",
    "    \n",
    "def batch_download(bucket: str, keys: [str], tmp_dir: Path) -> [Path]:\n",
    "    def curried_download(key: str): \n",
    "        return download_object(bucket,key,tmp_dir)\n",
    "    with ThreadPoolExecutor() as P:\n",
    "        return list(P.map(curried_download,keys))\n",
    "\n",
    "    \n",
    "def clean_tmp_files(paths: [Path]):\n",
    "    [Path(p).unlink() for p in paths]\n",
    "    \n",
    "    \n",
    "def load_image(path: Path) -> np.ndarray:\n",
    "    img = Image.open(path)\n",
    "    return np.array(img)\n",
    "\n",
    "    \n",
    "def load_images(paths: [Path]) -> np.ndarray:\n",
    "    return np.concatenate([\n",
    "        np.expand_dims(load_image(p),0)\n",
    "        for p in paths\n",
    "    ],0)\n",
    "\n",
    "\n",
    "def format_input(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Rescale from [0,255] to [0.0,1.0]\"\"\"\n",
    "    return data.astype(\"float32\") / 255\n",
    "\n",
    "\n",
    "def vgg_process(data: np.ndarray):\n",
    "    res = vgg16.predict(data)\n",
    "    return res\n",
    "\n",
    "\n",
    "def format_output(data: np.ndarray) -> np.ndarray:\n",
    "    batch_size, *_ = data.shape\n",
    "    return data.reshape((batch_size, -1))\n",
    "\n",
    "\n",
    "def make_arrow_table(row: np.ndarray, filename: Path) -> pa.Table:\n",
    "    return pa.table({Path(filename).stem: row})\n",
    "\n",
    "\n",
    "def write_parquet(row: np.ndarray, filename: Path) -> Path:\n",
    "    table = make_arrow_table(row, filename)\n",
    "    new_filename = filename.with_suffix(\".parquet\")\n",
    "    pq.write_table(table, new_filename)\n",
    "    return new_filename\n",
    "\n",
    "\n",
    "def write_parquets(data: np.ndarray, filenames: [Path]) -> [Path]:\n",
    "    return [write_parquet(r, f) for r, f in zip(data, filenames)]\n",
    "\n",
    "\n",
    "def upload_parquet_files(bucket: str, filenames: [Path]):\n",
    "    for filename in filenames:\n",
    "        key = filename.name\n",
    "        s3.upload_file(str(filename), bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = dt.datetime.now()\n",
    "print(f\"START TIME: {start_time}\")\n",
    "print(f\"Loading batches of {batch_size:,d} images.\\n\")\n",
    "\n",
    "for i, image_keys in enumerate(iter_keys(SOURCE_BUCKET, batch_size)):\n",
    "    print(f\"[{i:4,d}] Batch starting.\")\n",
    "    print(\"> Downloading images...\")\n",
    "    s = dt.datetime.now()\n",
    "    image_paths = batch_download(SOURCE_BUCKET, image_keys, tmp_dir)\n",
    "    print(f\"  TIME TO COMPLETE: {dt.datetime.now() - s}\")\n",
    "    print(\"> Loading into array...\")\n",
    "    input_data = load_images(image_paths)\n",
    "    input_data = format_input(input_data)\n",
    "    print(\"> Removing local image files...\")\n",
    "    clean_tmp_files(image_paths)\n",
    "    print(\"> VGG encoding images...\")\n",
    "    s = dt.datetime.now()\n",
    "    encoding = vgg_process(input_data)\n",
    "    print(f\"  TIME TO COMPLETE: {dt.datetime.now() - s}\")\n",
    "    output_data = format_output(encoding)\n",
    "    print(\"> Saving to parquet...\")\n",
    "    parquet_paths = write_parquets(output_data, image_paths)\n",
    "    print(\"> Uploading encodings...\")\n",
    "    s = dt.datetime.now()\n",
    "    upload_parquet_files(DEST_BUCKET, parquet_paths)\n",
    "    print(f\"  TIME TO COMPLETE: {dt.datetime.now() - s}\")\n",
    "    print(\"> Removing local parquet files...\")\n",
    "    clean_tmp_files(parquet_paths)\n",
    "    print(\"> Complete.\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFULL TIME TO COMPLETE: {dt.datetime.now() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd01ee9554a41884410edf6e256c19a4ea695ccaead463942267e87d57e486e7851",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}